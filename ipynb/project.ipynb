{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from datetime import date\n",
    "import calendar\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "from IPython.display import display, HTML\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "%matplotlib inline\n",
    "\n",
    "cd = os.path.split(os.getcwd())[0]\n",
    "if cd not in sys.path:\n",
    "    sys.path.append(cd)\n",
    "\n",
    "from lib import noaa, bexarcrime\n",
    "\n",
    "\n",
    "# Set this to true if you want to run it from scratch\n",
    "# IE pulling all the data from source and running all the slow\n",
    "# functions\n",
    "PROCESS_FULLY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [City Selection](#selection)\n",
    "  * [Loading](#loading)\n",
    "  * [Merging](#merging)\n",
    "  * [Visualisations](#visualisations)\n",
    "* [Data Acquisition](#acquisition)\n",
    "* [Data Analysis](#analysis)\n",
    "  * [Hypthesis](#hypothesis)\n",
    "  * [Exploration](#exploration)\n",
    "* [Results and Conclusions](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County-level crime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    # using crime reports, not arrests \n",
    "    crime = pd.read_csv('../data/CountyCrimeReports.tsv', sep='\\t')\n",
    "    crime['FIPS'] = crime['FIPS_ST'] * 1000 + crime['FIPS_CTY']\n",
    "    crime['vcrime'] = crime['MURDER'] + crime['RAPE'] + crime['ROBBERY'] + crime['AGASSLT']\n",
    "    crime = crime.set_index('FIPS')\n",
    "    crime = crime[['COVIND', 'vcrime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    edu = pd.read_excel('../data/Education.xls', skiprows=4)\n",
    "\n",
    "    # state and areas are named nicely in this dataset and will be kept for the later 'join'\n",
    "    # columns[-4:] include most recent data for adults eduction\n",
    "    # I chose the most recent because its not like the total number of HS dropouts is going to change THAT much\n",
    "    edu = edu[['FIPS Code', 'State', 'Area name'] + list(edu.columns[-4:])]\n",
    "    edu.rename(columns={'FIPS Code':'FIPS', \\\n",
    "                        'Area name':'County',\\\n",
    "                        'Percent of adults with less than a high school diploma, 2011-2015':'p_no_HS_dip', \\\n",
    "                        'Percent of adults with a high school diploma only, 2011-2015':'p_HS_dip',\\\n",
    "                        'Percent of adults completing some college or associate\\'s degree, 2011-2015':'p_some_college',\\\n",
    "                        'Percent of adults with a bachelor\\'s degree or higher, 2011-2015':'p_college_dip'}, inplace=True)\n",
    "    edu = edu.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Population dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    pop = pd.read_excel('../data/PopulationEstimates.xls', skiprows=2)\n",
    "\n",
    "    # average the columns\n",
    "    cols = ['POP_ESTIMATE_2010','POP_ESTIMATE_2011','POP_ESTIMATE_2012','POP_ESTIMATE_2013','POP_ESTIMATE_2014','POP_ESTIMATE_2015','POP_ESTIMATE_2016']\n",
    "    pop['avgpop'] = pop[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "    # more averaging\n",
    "    cols = ['N_POP_CHG_2010','N_POP_CHG_2011','N_POP_CHG_2012','N_POP_CHG_2013','N_POP_CHG_2014','N_POP_CHG_2015','N_POP_CHG_2016']\n",
    "    pop['dpop/dt'] = pop[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "    # only pull FIPS code, population, and dp\n",
    "    pop = pop[['FIPS', 'avgpop', 'dpop/dt']]\n",
    "    pop = pop.set_index('FIPS')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poverty estimate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    pov = pd.read_excel('../data/PovertyEstimates.xls', skiprows=3)\n",
    "    # only select poverty percentage\n",
    "    pov = pov[['FIPStxt', 'PCTPOVALL_2015']]\n",
    "    pov.rename(columns={'FIPStxt':'FIPS', 'PCTPOVALL_2015':'p_impoverished'}, inplace=True)\n",
    "    pov = pov.set_index('FIPS')\n",
    "    pov.p_impoverished = pd.to_numeric(pov.p_impoverished, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment estimates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    emp = pd.read_excel('../data/Unemployment.xls', skiprows=9)\n",
    "\n",
    "    #avg unemployment\n",
    "    cols = ['Unemployment_rate_2007', 'Unemployment_rate_2008', 'Unemployment_rate_2009', 'Unemployment_rate_2010', 'Unemployment_rate_2011', 'Unemployment_rate_2012', 'Unemployment_rate_2013', 'Unemployment_rate_2014', 'Unemployment_rate_2015', 'Unemployment_rate_2016']\n",
    "    emp['p_unempl'] = emp[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "    #only pull average and income\n",
    "    emp = emp[['FIPStxt', 'p_unempl', 'Median_Household_Income_2015']]\n",
    "    emp.rename(columns={'FIPStxt':'FIPS', 'Median_Household_Income_2015':'med_income'}, inplace=True)\n",
    "    emp = emp.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging <a class=\"anchor\" id=\"merging\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the datasets into a single one, and take out country- and state-wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    df = edu.join([pop,pov,emp,crime], how='outer')\n",
    "    df = df.where(df.State != 'PR').dropna(how='all') ## Puerto Rico has unreliable data\n",
    "\n",
    "    #pull out nationwide data\n",
    "    us = df.iloc[0]\n",
    "    df = df.drop(0)\n",
    "    \n",
    "    #pull out statewide data\n",
    "    s = [x for x in range(1000,75000,1000)]\n",
    "    states = df.loc[s].dropna(how='all')\n",
    "\n",
    "    # all thats left is county level data\n",
    "    df = df.drop(states.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the merged datasets, violent crime is violentcrime per 100,00 residents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    #normalizing data\n",
    "    df['p_dpop'] = df['dpop/dt']/df['avgpop']\n",
    "    df['vcrime_rate'] = 100000 * df['vcrime']/df['avgpop']\n",
    "    df = df.drop(['dpop/dt', 'vcrime'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations <a class=\"anchor\" id=\"visualisations\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics of the county-level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "        display(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "        display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of different states, with bonus political standpoints according the the 2008 presidential election between Barack Obama and John McCain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot of violent crime rate per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    # According to the 2008 presidential election\n",
    "    blue_states =['WA', 'OR', 'CA', 'NV', 'NM', 'CO', 'MN', 'IA', 'WI', 'IL', 'IN', 'MI', 'OH', 'PA', 'NY', 'VT', 'NH', 'ME', 'MA', 'CT', 'RI', 'NJ', 'DE', 'MD', 'VA', 'NC', 'FL', 'HI']\n",
    "    red_states = ['ID', 'MT', 'WY', 'UT', 'AZ', 'ND', 'SD', 'NE', 'KS', 'OK', 'TX', 'MO', 'AR', 'LA', 'WV', 'KY', 'TN', 'MS', 'AL', 'GA', 'SC', 'AK']\n",
    "    fix, ax = plt.subplots(figsize=(20,10))\n",
    "    pal = {state: 'r' if state in red_states else \"b\" for state in df.State}\n",
    "    sns.boxplot(ax=ax, x='State', y='vcrime_rate', data=df, palette=pal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot of percentages without a highschool diploma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    sns.boxplot(ax=ax, x='State', y='p_no_HS_dip', data=df, palette=pal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs of factors to violent crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    sns.pairplot(df, y_vars=['vcrime_rate'], x_vars=['p_no_HS_dip', 'p_HS_dip', 'p_some_college', 'p_college_dip', 'avgpop',\n",
    "       'p_impoverished', 'p_unempl', 'med_income', 'p_dpop', 'vcrime_rate'], dropna=True, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    sns.distplot(df.vcrime_rate.dropna(), axlabel=\"Violent crime per 100,000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    sns.distplot(df.avgpop.dropna().apply(np.log10), axlabel=\"Population (log10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    sns.distplot(df.p_unempl.dropna(), axlabel='Unemployment Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    sns.distplot(df.p_impoverished.dropna(), axlabel=\"Poverty Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin the data into high, medium and low (based on national quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    binned = pd.DataFrame({c : pd.qcut(df[c], 3, labels=['L', 'M', 'H']) for c in df.drop(['State', 'County', 'COVIND'], axis=1).columns}).join(df[['State', 'County', 'COVIND']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of the five worst counties in Texas (based on high rates of unemployment, crime, and population. Half of them are border towns with immigration problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    TX = binned.dropna(how='all').groupby(['vcrime_rate', 'p_unempl', 'avgpop'])\n",
    "    display(df.loc[TX.get_group(('H', 'H', 'H')).index].where(df.State == 'TX').dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of how Texas matches up nationwide to crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    sns.distplot(df.where(df.State=='TX').vcrime_rate.dropna(), label=\"Violent Crime Rates in Texas\")\n",
    "    sns.distplot(df.vcrime_rate.dropna(), axlabel=\"Violent Crime Rates in US and Texas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of the highest crime rates in the US with at least a population of 10000 to cull outliers. Note that high city crime does not necessarily match high county crime. For example, Chicago is a high crime city, but because it's split between two counties it's ranked lower on this list. St. Louis, however, is both a city and its own county, so it's data is more precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    display(df.where(df.avgpop > 10000).sort_values('vcrime_rate', ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grouped the data by violent crime rate, poverty rate, unemployment rate, and population.\n",
    "\n",
    "We select counties with high rates of enemployment, violent crime, poverty, and large populations sampled using a nonrandom seed for consistency between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    # all counties grouped by H/M/L rates of each factor\n",
    "    c = binned.dropna(how='all').groupby(groups[::-1])\n",
    "    display(c.count().where(c.count().State > 10).dropna().sort_values('State', ascending=False)['State'].unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    groups = ['vcrime_rate', 'p_impoverished', 'p_unempl', 'avgpop']\n",
    "    \n",
    "    selection = ('H','H','H', 'H')\n",
    "    for x in groups:\n",
    "        print(\"%10s \" %x[:10], end='')\n",
    "    print('')\n",
    "    for x in selection:\n",
    "        print(\"%10s \" %x[:10], end='')\n",
    "    HHHstates = df.loc[c.get_group(selection).index]\n",
    "    display(HHHstates.where(HHHstates.vcrime_rate > 800).dropna().sample(10, random_state=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those counties, we selected the county seats as the cities\n",
    "We looked up the latitude and longitude of them to match to NOAA's list of weather stations. We used Pythagoras' theorem to find the closest station to the city, as some cities may not have one within city limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    cities = {\n",
    "        'Philidelphia, PA' : (39.9526, -75.1652), #Philadelphia County\n",
    "        'Albany, GA' : (31.5785, -84.1557), # Gougherty County\n",
    "        'Memphis, TN' : (35.1495, -90.0490), # Shelby County and Crittenden County\n",
    "        'Toledo, OH' : (41.6639, -83.5552), # Lucas County\n",
    "        'Pine Bluff AR' : (34.2284, -92.0032), # Jefferson County\n",
    "        'Detroit, MI' : (42.3314, -83.0458), # Wayne County\n",
    "        'Baltimore, MD' : (39.2904, -76.6122), # Baltimore City\n",
    "        'Flint, MI' : (43.0125, -83.6875), # Genesee County\n",
    "        'St. Louis, MO' : (38.6270, -90.1994) # St. Louis City\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we cross-referenced those locations with the NOAA ISD dataset to find the nearest stations.\n",
    "\n",
    "The stations were filtered such that we only selected stations that had recent (more recent than 2012) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    # list of stations with location, name, and recording beginning and end dates\n",
    "    hist = pd.read_csv('ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv')\n",
    "    # only recent stations\n",
    "    hist = hist.where(hist.END > 20120101 ).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined some helper functions to process the station codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return math.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_station_code(usaf, wban):\n",
    "    usafstr = str(int(usaf))\n",
    "    wbanstr = str(int(wban))\n",
    "    \n",
    "    if len(usafstr) < 6:\n",
    "        usafstr = '0'*(6-len(usafstr)) + usafstr\n",
    "        \n",
    "    if len(wbanstr) < 5:\n",
    "        wbanstr = '0'*(5-len(wbanstr)) + wbanstr\n",
    "        \n",
    "    return usafstr + '-' + wbanstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    stations = dict()\n",
    "    for city in cities.keys():\n",
    "        coord = cities[city]\n",
    "        mindist = 999\n",
    "        minindex = 0\n",
    "        for index, row in hist.iterrows():\n",
    "            d = dist(coord, (row['LAT'], row['LON']))\n",
    "            if (d < mindist):\n",
    "                mindist = d\n",
    "                minindex = index\n",
    "        print('Nearest ({:^6.2f}) ISD to {:20} is {:40} at loc {}'.format(mindist, city, hist.loc[minindex]['STATION NAME'], minindex))\n",
    "        stations[city] = format_station_code(hist.loc[minindex]['USAF'], hist.loc[minindex]['WBAN'])\n",
    "        print('\\tStation code is {}'.format(stations[city]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition <a class=\"anchor\" id=\"acquisition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions to download crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(city):\n",
    "    \"\"\"Returns BeautifulSoup object for each set of links\"\"\"\n",
    "    r = requests.get('https://spotcrime.com/' + city + 'daily')\n",
    "    r2 = requests.get('https://spotcrime.com/' + city + 'daily/more')\n",
    "    soup1 = BeautifulSoup(r.text, 'html.parser')\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    \n",
    "    return soup1, soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(soups):\n",
    "    \"\"\"Pulls all the links from each BeautifulSoup object into a single list\"\"\"\n",
    "    links = []\n",
    "    for dates in soups[0].find_all('ol', class_='list-unstyled'):\n",
    "        for link in dates.find_all('a'):\n",
    "            links.append(link['href'])\n",
    "    for dates in soups[1].find_all('ol', class_='list-unstyled'):\n",
    "        for link in dates.find_all('a'):\n",
    "            links.append(link['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crime_df(links, base_url):\n",
    "    \"\"\"Loads each link and downloads the table of crimes, storing it in a list of lists\n",
    "    Returns a dataframe\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, link in enumerate(links):\n",
    "        print(i, link)\n",
    "        try:\n",
    "            r = requests.get(base_url + link)\n",
    "        except:\n",
    "            print('uh oh, timeout')\n",
    "            time.sleep(10)\n",
    "            r = requests.get(base_url+link)\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if table == None:\n",
    "            print('no table, skipping')\n",
    "            continue\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            crime = [ele for ele in cols if ele]\n",
    "            if len(crime) == 0:\n",
    "                continue\n",
    "            if len(crime) == 4:\n",
    "                crime = ['A'] + crime\n",
    "            data.append(crime)\n",
    "    return pd.DataFrame(data, columns=['A', 'Crime', 'Time', 'Address', 'Details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://spotcrime.com'\n",
    "cities = [\n",
    "    'mi/detroit/',\n",
    "    'mo/st.+louis/',\n",
    "    'md/baltimore/',\n",
    "    'oh/toledo/',\n",
    "    'ga/albany/',\n",
    "    'mi/flint/',\n",
    "    'tn/memphis/',\n",
    "    'pa/philadelphia/',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all the crime data for each city and save it to a gzipped csv file in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    for city in cities:\n",
    "        soup1, soup2 = get_soup(city)\n",
    "\n",
    "        links = get_links((soup1, soup2))\n",
    "        df = crime_df(links, base_url)\n",
    "        df = df.drop(['A', 'Address', 'Details'], axis=1)\n",
    "        df.to_csv('../data/crime_{}_{}.csv.gz'.format(city[3:-1], city[:2]), compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class to hold the city datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class City:\n",
    "    \"\"\"Class representing each city\n",
    "    \n",
    "    Attributes:\n",
    "        name (str): Name of the city City, 2-letter State\n",
    "        filepath (str): Filepath for the crime data\n",
    "        isd_code (str): Code for NOAA's ISD. Composed of USAF-WBAN id\n",
    "        dfc (DataFrame): Dataframe holding all crime data for the city\n",
    "        dfv (DataFrame): Dataframe holding only violent crime for the city\n",
    "        dfw (DataFrame): Dataframe holding weather data for the city\n",
    "        df (Dataframe) : Dataframe holding the merged weather+violentcrime data\n",
    "        all_crime (DataFrame): Alias for dfc\n",
    "        violent_crime (DataFrame): Alias for dfv\n",
    "        weather (Dataframe): Alias for dfw\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, filepath, isd_code):\n",
    "        self.name = name\n",
    "        self.filepath = filepath\n",
    "        self.isd_code = isd_code\n",
    "    \n",
    "    def load_crime(self, process=True):\n",
    "        \"\"\"Reads crime dataset from filepath and stores in dfc and dfv\n",
    "        \n",
    "        Args:\n",
    "            process (bool): Whether to immediately or lazily process the data\n",
    "                Defaults to true, process the data\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.dfc = pd.read_csv(self.filepath, compression='gzip')\n",
    "        self.dfv = self.dfc.where(self.dfc.Crime.isin(['Assault', 'Robbery', 'Shooting', 'Arson'])).dropna()\n",
    "        self.all_crime = self.dfc\n",
    "        self.violent_crime = self.dfv\n",
    "        if process: \n",
    "            return self.process_crime()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def process_crime(self, how='Fast'):\n",
    "        \"\"\"Processes the crime by converting the dates to datetime dtypes\n",
    "        \n",
    "        Args:\n",
    "            how (str): How to process the crime. Fast method drops data that doesn't\n",
    "                            specify the time of day, and is in general faster\n",
    "                            \n",
    "        Returns:\n",
    "            self\n",
    "            \n",
    "        \"\"\"\n",
    "        if how == 'Fast':\n",
    "            self.dfc.Time = pd.to_datetime(self.dfc.Time, \n",
    "                                           format='%m/%d/%y. %I:%M %p.', \n",
    "                                           errors='coerce').dropna()\n",
    "            self.dfc = self.dfc[self.dfc.Time.notnull()]\n",
    "        else:\n",
    "            self.dfc.Time = pd.to_datetime(self.dfc.Time, errors='coerce')\n",
    "            \n",
    "        self.dfc = self.dfc.set_index('Time')\n",
    "        self.dfv = self.dfc.where(self.dfc.Crime.isin(['Assault', 'Robbery', 'Shooting', 'Arson'])).dropna()\n",
    "        self.all_crime = self.dfc\n",
    "        self.violent_crime = self.dfv\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def load_weather(self, start=2016, end=2018):\n",
    "        \"\"\"Loads weather over FTP from NOAA's website into dfw\n",
    "        \n",
    "        Args:\n",
    "            start (int): Start year \n",
    "            end (int): End year\n",
    "            \n",
    "        Returns:\n",
    "            self\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dfw = noaa.noaa_from_web(self.isd_code, start, end).fillna(method='backfill')\n",
    "            \n",
    "        # drop relative humididty \n",
    "        self.dfw = self.dfw.drop('RHPeriod', axis = 1)\n",
    "        \n",
    "        # replace null values\n",
    "        self.dfw['Temperature'] = self.dfw['Temperature'].replace(9999,np.nan)\n",
    "        self.dfw['Pressure'] = self.dfw['Pressure'].replace(99999,np.nan)\n",
    "        self.dfw['Humidity'] = self.dfw['Humidity'].replace(999, np.nan)\n",
    "        self.dfw['Sky'] = self.dfw['Sky'].replace([9,99], np.nan)\n",
    "        \n",
    "        # scale values back\n",
    "        self.dfw['Temperature'] = self.dfw['Temperature'].map(lambda x : x/10)\n",
    "        self.dfw['Pressure'] = self.dfw['Pressure'].map(lambda x : x/10)\n",
    "        \n",
    "        # map sky oktas to coverage percentages, roughly\n",
    "        self.dfw['Sky'] = self.dfw['Sky'].map(lambda x : x/8)\n",
    "        \n",
    "        # convert C to F\n",
    "        self.dfw['Temperature'] = self.dfw['Temperature'].map(lambda x : x * 9/5 + 32)\n",
    "        self.weather = self.dfw\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def merge_dfs(self, start='2016-01-01', end='2017-01-01'):\n",
    "        \"\"\"Merges violent crime and weather into a single dataset, df, cut into a range\n",
    "        \n",
    "        Args:\n",
    "            start (date string): Start date to cut\n",
    "            end (date string): End date to cut to\n",
    "            \n",
    "        Returns:\n",
    "            Self\n",
    "        \n",
    "        \"\"\"\n",
    "        self.df = self.dfw.join(self.dfv, how='outer')\n",
    "        self.df = self.df.groupby( \n",
    "                        [self.df.index.year, \n",
    "                         self.df.index.month, \n",
    "                         self.df.index.day, \n",
    "                         self.df.index.hour]\n",
    "                    ).agg ({   \n",
    "                         'Temperature' : 'mean', \n",
    "                         'Pressure' : 'mean',\n",
    "                         'Humidity' : 'mean',\n",
    "                         'Sky' : 'mean',\n",
    "                         'Crime' : 'count'}\n",
    "                    ).reset_index().rename(columns={\n",
    "                        'level_0':'year',\n",
    "                        'level_1':'month',\n",
    "                        'level_2':'day',\n",
    "                        'level_3':'hour',\n",
    "                    })\n",
    "        s = pd.to_datetime(self.df[['year', 'month', 'day', 'hour']])\n",
    "        self.df = self.df.set_index(s).drop(['year', 'month', 'day', 'hour'], \n",
    "                                    axis=1)\n",
    "        \n",
    "        self.df = self.df.loc[self.df.index > start]\n",
    "        self.df = self.df.loc[self.df.index < end]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = {\n",
    "    ##'Albany, GA': City('Albany, GA', '../data/crime_albany_ga.csv.gz', '722160-13869'),\n",
    "    ## Not enough data for Albany\n",
    "    'Baltimore, MD': City('Baltimore, MD', '../data/crime_baltimore_md.csv.gz', '745944-93784'),\n",
    "    'Detroit, MI': City('Detroit, MI', '../data/crime_detroit_mi.csv.gz', '725375-14822'),\n",
    "    'Flint, MI': City('Flint, MI', '../data/crime_flint_mi.csv.gz', '726370-14826'),\n",
    "    'Memphis, TN': City('Memphis, TN', '../data/crime_memphis_tn.csv.gz', '723340-13893'),\n",
    "    'Philadelphia, PA': City('Philadelphia, PA', '../data/crime_philadelphia_pa.csv.gz', '724080-13739'),\n",
    "    ##'Pine Bluff AR': \n",
    "    ## no data for Pine Bluff\n",
    "    'St. Louis, MO': City('St. Louis, MO', '../data/crime_st.+louis_mo.csv.gz', '725314-03960'),\n",
    "    'Toledo, OH': City('Toledo, OH', '../data/crime_toledo_oh.csv.gz', '720275-04872')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the crime data into the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    for city in cities.values():\n",
    "        city.load_crime().load_weather().merge_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses <a class=\"anchor\" id=\"hypothesis\"></a>\n",
    "* 1. Temperature is positively correlated with the violent crime rates (Richard)\n",
    "* 2. Humidity and pressure have no impact on violent crime\n",
    "* 3. Violent crime is higher in summer vs winter (Lalo)\n",
    "* 4. Violent crime is higher during midnight hours (11:00pm - 2:00am) (Lexi)\n",
    "* 5. Is there a spike in total crime at 2:00AM because bars close? (Laxo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration <a class=\"anchor\" id=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking time difference and data loss because of the format enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baltimore, MD\n",
      "Slow: 223.96318006515503 Fast: 3.2265207767486572\n",
      "Detroit, MI\n",
      "Slow: 79.27959275245667 Fast: 0.7898049354553223\n",
      "Flint, MI\n",
      "Slow: 1.5368225574493408 Fast: 0.012137174606323242\n",
      "Memphis, TN\n",
      "Slow: 188.08573937416077 Fast: 3.0978517532348633\n",
      "Philadelphia, PA\n",
      "Slow: 218.8193874359131 Fast: 3.94437837600708\n",
      "St. Louis, MO\n",
      "Slow: 65.05099153518677 Fast: 1.0785024166107178\n",
      "Toledo, OH\n",
      "Slow: 17.29680037498474 Fast: 0.23884344100952148\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baltimore, MD</th>\n",
       "      <th>Detroit, MI</th>\n",
       "      <th>Flint, MI</th>\n",
       "      <th>Memphis, TN</th>\n",
       "      <th>Philadelphia, PA</th>\n",
       "      <th>St. Louis, MO</th>\n",
       "      <th>Toledo, OH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arrest</th>\n",
       "      <td>0.994262</td>\n",
       "      <td>0.369246</td>\n",
       "      <td>0.037559</td>\n",
       "      <td>0.995202</td>\n",
       "      <td>0.991354</td>\n",
       "      <td>0.755795</td>\n",
       "      <td>0.525197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arson</th>\n",
       "      <td>0.515254</td>\n",
       "      <td>0.454158</td>\n",
       "      <td>0.025937</td>\n",
       "      <td>0.994919</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.682870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assault</th>\n",
       "      <td>0.637649</td>\n",
       "      <td>0.258590</td>\n",
       "      <td>0.073566</td>\n",
       "      <td>0.997853</td>\n",
       "      <td>0.997054</td>\n",
       "      <td>0.773747</td>\n",
       "      <td>0.472545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burglary</th>\n",
       "      <td>0.428944</td>\n",
       "      <td>0.264795</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.996855</td>\n",
       "      <td>0.996788</td>\n",
       "      <td>0.735583</td>\n",
       "      <td>0.343376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.267180</td>\n",
       "      <td>0.225458</td>\n",
       "      <td>0.998993</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.913931</td>\n",
       "      <td>0.989402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robbery</th>\n",
       "      <td>0.503708</td>\n",
       "      <td>0.570339</td>\n",
       "      <td>0.596610</td>\n",
       "      <td>0.995363</td>\n",
       "      <td>0.995316</td>\n",
       "      <td>0.786775</td>\n",
       "      <td>0.361761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shooting</th>\n",
       "      <td>0.849033</td>\n",
       "      <td>0.944949</td>\n",
       "      <td>0.383013</td>\n",
       "      <td>0.835112</td>\n",
       "      <td>0.911980</td>\n",
       "      <td>0.749254</td>\n",
       "      <td>0.846970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Theft</th>\n",
       "      <td>0.398938</td>\n",
       "      <td>0.278902</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.997238</td>\n",
       "      <td>0.998257</td>\n",
       "      <td>0.832724</td>\n",
       "      <td>0.580260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vandalism</th>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.195425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996840</td>\n",
       "      <td>0.999750</td>\n",
       "      <td>0.573438</td>\n",
       "      <td>0.765670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Baltimore, MD  Detroit, MI  Flint, MI  Memphis, TN  \\\n",
       "Arrest          0.994262     0.369246   0.037559     0.995202   \n",
       "Arson           0.515254     0.454158   0.025937     0.994919   \n",
       "Assault         0.637649     0.258590   0.073566     0.997853   \n",
       "Burglary        0.428944     0.264795   0.061728     0.996855   \n",
       "Other           0.999275     0.267180   0.225458     0.998993   \n",
       "Robbery         0.503708     0.570339   0.596610     0.995363   \n",
       "Shooting        0.849033     0.944949   0.383013     0.835112   \n",
       "Theft           0.398938     0.278902   0.004967     0.997238   \n",
       "Vandalism       0.995617     0.195425        NaN     0.996840   \n",
       "\n",
       "           Philadelphia, PA  St. Louis, MO  Toledo, OH  \n",
       "Arrest             0.991354       0.755795    0.525197  \n",
       "Arson              0.980000       0.999352    0.682870  \n",
       "Assault            0.997054       0.773747    0.472545  \n",
       "Burglary           0.996788       0.735583    0.343376  \n",
       "Other              0.999803       0.913931    0.989402  \n",
       "Robbery            0.995316       0.786775    0.361761  \n",
       "Shooting           0.911980       0.749254    0.846970  \n",
       "Theft              0.998257       0.832724    0.580260  \n",
       "Vandalism          0.999750       0.573438    0.765670  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PROCESS_FULLY:\n",
    "    datalosses = pd.DataFrame()\n",
    "\n",
    "    for city in cities.values():\n",
    "        print(city.name)\n",
    "\n",
    "        city.load_crime(False)\n",
    "        t1 = time.time()\n",
    "        city.process_crime(how='Slow')\n",
    "        t2 = time.time()\n",
    "        slow = t2 - t1\n",
    "        df1 = city.dfc\n",
    "\n",
    "        city.load_crime(False)\n",
    "        t1 = time.time()\n",
    "        city.process_crime(how='Fast')\n",
    "        t2 = time.time()\n",
    "        fast = t2-t1\n",
    "        df2 = city.dfc\n",
    "\n",
    "        print('Slow: {} Fast: {}'.format(slow, fast))\n",
    "        datalosses[city.name] = df2.Crime.value_counts()/df1.Crime.value_counts()\n",
    "\n",
    "    datalosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for city in cities.values():\n",
    "    print(city.name)\n",
    "    city.load_weather().load_crime().merge_dfs()\n",
    "    df = city.df[['Temperature', 'Crime']].dropna()\n",
    "    sns.jointplot(x='Temperature', y='Crime', data=df)\n",
    "    plt.title(city.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with times for baltimore and st louis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blt = cities['Baltimore, MD']\n",
    "blt.load_crime().load_weather().merge_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stl = cities['St. Louis, MO']\n",
    "stl.load_crime().load_weather().merge_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = blt.df[['Temperature', 'Pressure', 'Humidity', 'Crime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blt.dfc.Crime.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stl.dfc.Crime.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shooting = blt.dfc.where(blt.dfc.Crime == 'Shooting').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shooting.groupby(shooting.index.hour).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine if Hypothesis number four is correct/incorrect for Baltimore, MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = blt.dfv.between_time(start_time='23:00', end_time='23:59')\n",
    "dff = blt.dfv.between_time(start_time = '00:01', end_time = '02:00')\n",
    "df3 = blt.dfv.between_time(start_time = '02:00', end_time = '23:00')\n",
    "df4 = blt.dfv.between_time(start_time = '00:01', end_time = '23:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midHo = df.groupby(df.index.hour).count().sum() + dff.groupby(dff.index.hour).count().sum()\n",
    "otherHo = df3.groupby(df3.index.hour).count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = stl.dfc.between_time(start_time='00:01', end_time='23:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.where(df2.Crime == 'Theft').groupby(df2.index.hour).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Investigate Crime based on Day of the Week in Baltimore, MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a hack\n",
    "d = {'Monday' : 0, 'Tuesday' : 1, 'Wednesday' : 2, 'Thursday' : 3, 'Friday': 4 , 'Saturday' : 5, 'Sunday' : 6}\n",
    "\n",
    "# Baltimore, MD\n",
    "df4 = blt.dfv\n",
    "days = {}\n",
    "for val in df4.index:\n",
    "    day = calendar.day_name[val.weekday()]\n",
    "    days[day] = days.get(day, 0) + 1\n",
    "sorted_days = [w for w in sorted(days.items(), key=lambda x: d[x[0]])]\n",
    "plt.bar(range(len(days)), [day[1] for day in sorted_days], align = 'center', color = 'b')\n",
    "plt.xticks(range(len(days)), [day[0] for day in sorted_days])\n",
    "plt.title('Baltimore, MD Crimes per Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Crimes per Day')\n",
    "plt.show()\n",
    "# St. Louis, MO\n",
    "df4 = stl.dfv\n",
    "days = {}\n",
    "for val in df4.index:\n",
    "    day = calendar.day_name[val.weekday()]\n",
    "    days[day] = days.get(day, 0) + 1\n",
    "sorted_days = [w for w in sorted(days.items(), key=lambda x: d[x[0]])]\n",
    "plt.bar(range(len(days)), [day[1] for day in sorted_days], align = 'center', color = 'r')\n",
    "plt.xticks(range(len(days)), [day[0] for day in sorted_days])\n",
    "plt.title('St. Louis, Mo Crimes per Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Crimes per Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine if Hypothesis four is correct/incorrect for St. Louis, Mo and compare with Baltimore, MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = stl.dfv.between_time(start_time='23:00', end_time='23:59')\n",
    "df2 = stl.dfv.between_time(start_time = '00:01', end_time = '02:00')\n",
    "df3 = stl.dfv.between_time(start_time = '02:00', end_time = '23:00')\n",
    "df4 = stl.dfv.between_time(start_time = '00:01', end_time = '23:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smidHo = df.groupby(df.index.hour).count().sum() + df2.groupby(df2.index.hour).count().sum()\n",
    "sotherHo = df3.groupby(df3.index.hour).count().sum()\n",
    "print(\"St. Louis Midnight Hours : {}, Baltimore Midnight Hours : {}\".format(smidHo[0], midHo[0]))\n",
    "print(\"St. Louis All Other Hours : {}, Blatimore All Other Hours : {}\".format(sotherHo[0], otherHo[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for increase in all crimes from 1-3 am due to Bar Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = blt.dfc.between_time(start_time = '01:00', end_time = '03:00')\n",
    "df2 = stl.dfc.between_time(start_time = '01:00', end_time = '03:00')\n",
    "df.groupby(df.index.hour).count().plot(kind = 'bar', title = 'Baltimore, MD', color = 'g')\n",
    "df2.groupby(df2.index.hour).count().plot(kind = 'bar', title = 'St. Louis, MO', color = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Determine if there is a positive correlation between Temperature and Violent Crime for Balitmore, MD and St. Louis, MO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = blt.df[['Temperature', 'Pressure', 'Crime']] # get df\n",
    "df.corr().plot(kind = 'box', title = 'Baltimore, MD') # create graph of corr\n",
    "df.corr().plot(kind = 'bar', title = 'Baltimore, MD') # create graph of corr\n",
    "df.corr().plot(kind = 'line', title = 'Baltimore, MD') # create graph of corr\n",
    "print('Baltimore, MD', df.corr()) # print chart of corr\n",
    "df2 = stl.df[['Temperature', 'Pressure', 'Crime']] # get df \n",
    "df2.corr().plot(kind = 'box', title = 'St. Louis, MO') # create graph of corr\n",
    "df2.corr().plot(kind = 'bar', title = 'St. Louis, MO') # create graph of corr\n",
    "df2.corr().plot(kind = 'line', title = 'St. Louis, MO') # create graph of corr\n",
    "print('St. Louis, MO', df2.corr()) # print chart of corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Conclusions <a class=\"anchor\" id=\"results\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
